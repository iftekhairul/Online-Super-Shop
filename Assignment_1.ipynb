{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Assignment-1.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTUPPev--9Zo",
        "colab_type": "text"
      },
      "source": [
        "# CS 5683 - Big Data Analytics\n",
        "## Assignment - 1: Intro. to Spark and RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chf1cDTs-9Zp",
        "colab_type": "text"
      },
      "source": [
        "###### Use Google Colab to use this notebook\n",
        "###### Let's setup Spark first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bivR5fvf-9Zq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d6d73326-4847-42a7-a20c-dbd28cc235cd"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/b0/bf9020b56492281b9c9d8aae8f44ff51e1bc91b3ef5a884385cb4e389a40/pyspark-3.0.0.tar.gz (204.7MB)\n",
            "\u001b[K     |████████████████████████████████| 204.7MB 66kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 46.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044184 sha256=7e98f6997c5b02258774b678ad22bfe2f991a96a4d308530bc207ee856252f83\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/27/4d/ddacf7143f8d5b76c45c61ee2e43d9f8492fc5a8e78ebd7d37\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLAWkTFh-9Zt",
        "colab_type": "text"
      },
      "source": [
        "###### Import required libraries now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PhP6FO8-9Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        " \n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fv0Qcxa-9Zw",
        "colab_type": "text"
      },
      "source": [
        "###### Let's initialize Spark context now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8JTG9r0-9Zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create Spark context with necessary configuration\n",
        "sc = SparkContext(\"local\",\"PySpark Word Count Exmaple\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmEu7ZJG-9Zy",
        "colab_type": "text"
      },
      "source": [
        "###### Follow the tutorial to mount your Google Drive. Give mounted Drive paths below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m2EiShR_7O7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "381ce8b2-e469-48c8-cb0a-68c8410f129c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iap8jXtW-9Zz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Give **.txt FILE PATHS** here\n",
        "file1 = '/content/drive/My Drive/BigDataAnalytics/Assignment1/file1.txt'\n",
        "file2 = '/content/drive/My Drive/BigDataAnalytics/Assignment1/file2.txt'\n",
        "\n",
        "# USE THESE FILES as input(s) FOR ALL BELOW QUESTIONS"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGvrYdvL-9Z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTqA21gf-9Z3",
        "colab_type": "text"
      },
      "source": [
        "### Example Spark program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTmBo-19-9Z4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example Spark application for a simple wordcount\n",
        "# What is wordcount? \n",
        "    # Given a file, count the frequency of all words appearing in that file\n",
        "    \n",
        "# Step-1: Read the required file. In our case it is file1 or file2.\n",
        "# NOTE: We do not need to initialize SparkContext as only one SparkContext can be initialized in one notebook\n",
        "fileRDD = sc.textFile(file1)\n",
        "#wordsRDD = fileRDD.flatMap(lambda line: line.split(\".\"))\n",
        "# Step-2: \n",
        "    # Each line in our file(s) is a sentence. So, we need to split the sentence with ' ' to get words\n",
        "    # Using map() will return RDD[list]. But we need RDD[string]. So we use flatMap()\n",
        "wordsRDD = fileRDD.flatMap(lambda line: line.split(\" \")) # <----------- TEST what happens when you use map()\n",
        "\n",
        "# Step-3: For each input, we will make (K,V) pair, where K is the word and V is 1\n",
        "pairRDD = wordsRDD.map(lambda word: (word,1))\n",
        "\n",
        "# Step-4: Now we have to sum all 1's of each word\n",
        "# NOTE: A word may present in multiple data partitions. So we use reduceByKey() to group by key and perform sum\n",
        "countRDD = pairRDD.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "#Step-5: Save results in a text file\n",
        "countRDD.saveAsTextFile('/content/drive/My Drive/BigDataAnalytics/Assignment1/example') # <----------- GIVE FILE PATH"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCpIPrT1-9Z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5Mmek9J-9Z_",
        "colab_type": "text"
      },
      "source": [
        "### Question - 1 (10 points) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rre7g6r6-9Z_",
        "colab_type": "raw"
      },
      "source": [
        "Write a Spark application that counts number of words that ends with each letter in file1. That is for each letter count the number of (non-unique) words that ends with the specific letter. You can **ignore** letter cases (consider the given text contains only lower-case letters). Also, you can **ignore** words that end with non-alphabetic letters. **Sort the output in alphabetical order**\n",
        "\n",
        "Example Output:\n",
        "('a', 500)\n",
        "('B', 100)\n",
        "\n",
        "which means that the given input has 500 words that end with the letter 'a' and 100 words that end with letter 'B'.\n",
        "NOTE: The output is sorted, the application counts duplicate words also, and the application is not case-sensitive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zthyAAB-9Z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YOUR CODE for Question-1 HERE \n",
        "#Filter the null value from word list \n",
        "wordsRDD2=wordsRDD.filter (lambda x: x!='')\n",
        "#convert all character into lowercase\n",
        "wordsRDD3=wordsRDD2.map (lambda x: x.lower())\n",
        "#get the last character of the word and map them with 1\n",
        "lastCharRDD = wordsRDD3.map(lambda word: (word[-1],1))\n",
        "# sum all 1 reducebykey function\n",
        "countLastChar = lastCharRDD.reduceByKey(lambda a,b: a+b)\n",
        "countLastCharAcc=countLastChar.sortByKey(ascending=True)\n",
        "countLastCharAcc.saveAsTextFile('/content/drive/My Drive/BigDataAnalytics/Assignment1/question1')\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbQxt29n-9aB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e9e46z0-9aD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PRINT THE OUTPUT HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWTVFD-k-9aG",
        "colab_type": "text"
      },
      "source": [
        "### Question - 2 (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pujKL29h-9aG",
        "colab_type": "raw"
      },
      "source": [
        "Write a Spark application that counts the **unique** number of words that have **n-letters** in file1. That is count the number of **unique** 1-letter words, 2-letter words, 3-letter words, etc. You can **include** all non-alphabetic letters for this application.\n",
        "\n",
        "Example Output:\n",
        "(1, 100)\n",
        "(2, 700)\n",
        "(3, 1500)\n",
        "\n",
        "which means that the input file for the Spark application has **100 unique 1-letter words** , **700 unique 2-letter words** , and **1500 uniqe 3-letter words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zzg_2GJ-9aG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YOUR CODE for Question-2 HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xHZDjW4-9aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the unique vlue from the RDD\n",
        "DistinctWordRDD=wordsRDD3.distinct()\n",
        "#Get the length of unque word and map them with 1 \n",
        "WordLenRDD = DistinctWordRDD.map(lambda word: (len(word),1))\n",
        "#sum all 1 from map \n",
        "countRDD4 = WordLenRDD.reduceByKey(lambda a,b: a+b)\n",
        "#sort the order\n",
        "countRDD5=countRDD4.sortByKey(ascending=True)\n",
        "#save the output on file\n",
        "countRDD5.saveAsTextFile('/content/drive/My Drive/BigDataAnalytics/Assignment1/question2')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL3MiHoE-9aK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PRINT THE OUTPUT HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-h1Hlpl-9aM",
        "colab_type": "text"
      },
      "source": [
        "### Question - 3 (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y4pUOKm-9aM",
        "colab_type": "raw"
      },
      "source": [
        "Write a Spark aplication that outputs wordcount from two files: file1 and file2. That is count the number of occurances of words from two files. You can **ignore** letter cases (consider the given input files contain only lower-case letters). Also, you can **ignore** that is not present in both files.\n",
        "\n",
        "Example Output:\n",
        "(big, (10, 20))\n",
        "(Data, (30, 50))\n",
        "\n",
        "which means the word \"big\" appears 10 times in file-1 and 20 times in file-2 and the word \"Data\" appears 30 times in file-1 and 50 times in file-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrWgOjrk-9aN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YOUR CODE for Question-3 HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCai1wxJ-9aO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the second file\n",
        "fileRDD2 = sc.textFile(file2)\n",
        "#split the line using whitespace\n",
        "wordsRDDf2 = fileRDD2.flatMap(lambda line: line.split(\" \")) \n",
        "#convert all character into lowercase\n",
        "wordslower=wordsRDDf2.map (lambda x: x.lower())\n",
        "# Step-3: For each input, we will make (K,V) pair, where K is the word and V is 1\n",
        "pairRDDf2 = wordslower.map(lambda word: (word,1))\n",
        "\n",
        "countRDDf2 = pairRDDf2.reduceByKey(lambda a,b: a+b)\n",
        "#join 2 RDD from file1 and file2\n",
        "finalRDD=countRDD.join(countRDDf2)\n",
        "#Step-3: For each input, we will make (K,(v1,v2)) pair, where K is the word and v1 is word frequency in file1 and v2 is word frequency in file2\n",
        "finalRDD2=finalRDD.reduceByKey(lambda a,b: (a,b))\n",
        "finalRDD2.saveAsTextFile('/content/drive/My Drive/BigDataAnalytics/Assignment1/question3')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWid5JM3-9aQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PRINT THE OUTPUT HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fB82Q8V-9aS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR7zXw0w-9aU",
        "colab_type": "text"
      },
      "source": [
        "### WHAT TO TURN-IN IN CANVAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiPCkm6k-9aU",
        "colab_type": "raw"
      },
      "source": [
        "1. Complete questions 1,2, and 3\n",
        "2. Download this completed python notebook as .ipynb\n",
        "3. Upload it in Canvas assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGaaRJxE-9aU",
        "colab_type": "text"
      },
      "source": [
        "# Due Date: Sept. 3 at 11:59pm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04ytXxJ0-9aV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}